{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook : Optimisation Avancée des Hyperparamètres pour Random Forest\n",
    "\n",
    "**Objectifs :**\n",
    "1. Optimiser les hyperparamètres du modèle **Random Forest** en utilisant **RandomizedSearchCV** pour explorer une plus large gamme de valeurs sans augmenter la taille de l'échantillon (20% des données).\n",
    "2. Utiliser une **validation croisée** pour évaluer les performances du modèle et vérifier sa stabilité.\n",
    "3. Vérifier si le modèle atteint le seuil de performance souhaité (R² ≥ 0.95).\n",
    "\n",
    "---\n",
    "\n",
    "## Étape 1 : Importer les bibliothèques nécessaires et charger les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Étape 2 : Charger les données et préparer un échantillon de 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données complètes\n",
    "file_path = 'note-books/phase_2_training/04_correlation_analysis/data_final_features.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Encoder `nutriscore_grade` en valeurs numériques\n",
    "df['nutriscore_grade_encoded'] = df['nutriscore_grade'].map({'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4})\n",
    "\n",
    "# Préparer les variables explicatives et la cible\n",
    "X = df[['fat_100g', 'saturated-fat_100g', 'energy_100g', 'energy-kcal_100g', 'sugars_100g']]\n",
    "y = df['nutriscore_grade_encoded']\n",
    "\n",
    "# Prendre un échantillon de 20% des données pour l’entraînement\n",
    "X_sample, _, y_sample, _ = train_test_split(X, y, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 3 : Définir la pipeline et les hyperparamètres pour RandomizedSearchCV\n",
    "\n",
    "Nous définissons une pipeline avec une étape de standardisation et le modèle **Random Forest**, puis nous créons une grille de recherche plus large pour les hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=100; total time=   5.5s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=100; total time=   5.6s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=100; total time=   5.6s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=100; total time=   5.6s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=100; total time=   5.7s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=300; total time=  11.5s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=300; total time=  11.5s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=300; total time=  11.6s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=300; total time=  11.6s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=300; total time=  11.6s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=300; total time=  17.6s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=300; total time=  17.7s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=300; total time=  17.8s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=300; total time=  17.9s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=300; total time=  17.9s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=500; total time=  28.5s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=500; total time=  28.5s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=500; total time=  28.6s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=500; total time=  28.7s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=500; total time=  28.6s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=6, model__min_samples_split=5, model__n_estimators=200; total time=  10.5s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=6, model__min_samples_split=5, model__n_estimators=200; total time=  10.5s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=6, model__min_samples_split=5, model__n_estimators=200; total time=  10.6s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=6, model__min_samples_split=5, model__n_estimators=200; total time=  10.5s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=6, model__min_samples_split=5, model__n_estimators=200; total time=  10.6s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=6, model__min_samples_split=10, model__n_estimators=200; total time=  10.5s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=6, model__min_samples_split=10, model__n_estimators=200; total time=  10.5s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=6, model__min_samples_split=10, model__n_estimators=200; total time=  10.7s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=6, model__min_samples_split=10, model__n_estimators=200; total time=  10.5s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=6, model__min_samples_split=10, model__n_estimators=200; total time=  10.8s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=6, model__min_samples_split=2, model__n_estimators=300; total time=  16.4s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=6, model__min_samples_split=2, model__n_estimators=300; total time=  16.3s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=6, model__min_samples_split=2, model__n_estimators=300; total time=  16.5s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=6, model__min_samples_split=2, model__n_estimators=300; total time=  16.3s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=6, model__min_samples_split=2, model__n_estimators=300; total time=  16.3s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   8.2s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   8.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   8.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   8.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   8.1s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=500; total time=  29.5s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=500; total time=  29.3s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=500; total time=  29.3s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=500; total time=  29.6s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=500; total time=  29.8s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=15, model__n_estimators=300; total time=  12.6s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=15, model__n_estimators=300; total time=  12.7s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=15, model__n_estimators=300; total time=  12.6s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=15, model__n_estimators=300; total time=  12.6s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=15, model__n_estimators=300; total time=  12.6s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=6, model__min_samples_split=5, model__n_estimators=100; total time=   5.7s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=6, model__min_samples_split=5, model__n_estimators=100; total time=   5.8s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=6, model__min_samples_split=5, model__n_estimators=100; total time=   5.8s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=6, model__min_samples_split=5, model__n_estimators=100; total time=   5.8s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=6, model__min_samples_split=5, model__n_estimators=100; total time=   5.7s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=15, model__n_estimators=300; total time=  20.3s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=15, model__n_estimators=300; total time=  20.3s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=15, model__n_estimators=300; total time=  20.5s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=15, model__n_estimators=300; total time=  20.4s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=15, model__n_estimators=300; total time=  20.3s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=300; total time=  20.6s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=300; total time=  20.5s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=300; total time=  20.6s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=300; total time=  20.4s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=300; total time=  20.5s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=100; total time=   6.4s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=100; total time=   6.5s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=100; total time=   6.5s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=100; total time=   6.8s\n",
      "[CV] END model__max_depth=30, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=100; total time=   6.5s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  12.6s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  12.8s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  12.5s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  12.6s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  12.7s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=100; total time=   6.4s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=100; total time=   6.5s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=100; total time=   6.6s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=100; total time=   6.4s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=100; total time=   6.6s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=500; total time=  20.8s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=500; total time=  20.7s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=500; total time=  20.8s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=500; total time=  20.6s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=15, model__n_estimators=500; total time=  21.1s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=100; total time=   5.8s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=100; total time=   6.1s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=100; total time=   6.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=100; total time=   6.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=100; total time=   6.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=500; total time=  33.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=500; total time=  33.4s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=500; total time=  33.4s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=500; total time=  33.5s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=100; total time=   4.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=500; total time=  33.6s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=100; total time=   4.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=100; total time=   4.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=100; total time=   4.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=100; total time=   3.8s\n",
      "Meilleurs paramètres trouvés avec Random Search : {'model__n_estimators': 500, 'model__min_samples_split': 15, 'model__min_samples_leaf': 1, 'model__max_depth': 20}\n"
     ]
    }
   ],
   "source": [
    "# Créer une pipeline avec une étape de standardisation et le modèle Random Forest\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Définir une grille de recherche étendue pour les hyperparamètres\n",
    "param_dist = {\n",
    "    'model__n_estimators': [100, 200, 300, 500],\n",
    "    'model__max_depth': [10, 20, 30, None],\n",
    "    'model__min_samples_split': [2, 5, 10, 15],\n",
    "    'model__min_samples_leaf': [1, 2, 4, 6]\n",
    "}\n",
    "\n",
    "# Configurer RandomizedSearchCV pour explorer les hyperparamètres\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,  # Nombre d'itérations pour limiter le temps d'exécution\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Entraîner le modèle avec Random Search sur l'échantillon de 20% des données\n",
    "random_search.fit(X_sample, y_sample)\n",
    "\n",
    "# Extraire les meilleurs paramètres\n",
    "best_params = random_search.best_params_\n",
    "print(\"Meilleurs paramètres trouvés avec Random Search :\", best_params)\n",
    "\n",
    "# Sauvegarder les meilleurs paramètres pour un suivi ultérieur\n",
    "output_folder = 'note-books/phase_2_training/09_random_forest_advanced_optimization'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "best_params_path = f'{output_folder}/best_hyperparameters.json'\n",
    "with open(best_params_path, 'w') as f:\n",
    "    json.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 4 : Évaluer les performances avec validation croisée\n",
    "\n",
    "Nous évaluons les performances du modèle optimisé en utilisant une **validation croisée** pour obtenir une estimation robuste de ses performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation croisée sur 5 plis - Score R² moyen : 0.6593, Écart-type : 0.0063\n",
      "Le modèle n'atteint toujours pas le seuil de performance souhaité. Envisagez des options supplémentaires.\n"
     ]
    }
   ],
   "source": [
    "# Utiliser le meilleur modèle trouvé par Random Search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Évaluer le modèle optimisé avec validation croisée\n",
    "kfold = 5  # Utiliser 5 plis pour la validation croisée\n",
    "scores = cross_val_score(best_model, X_sample, y_sample, cv=kfold, scoring='r2')\n",
    "mean_score = np.mean(scores)\n",
    "std_dev = np.std(scores)\n",
    "\n",
    "print(f\"Validation croisée sur {kfold} plis - Score R² moyen : {mean_score:.4f}, Écart-type : {std_dev:.4f}\")\n",
    "\n",
    "# Vérification du seuil de performance\n",
    "if mean_score >= 0.95:\n",
    "    print(\"Le modèle atteint le seuil de performance souhaité avec la validation croisée (R² moyen ≥ 0.95).\")\n",
    "else:\n",
    "    print(\"Le modèle n'atteint toujours pas le seuil de performance souhaité. Envisagez des options supplémentaires.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 5 : Sauvegarder le modèle optimisé et ses paramètres\n",
    "\n",
    "Si le modèle atteint le seuil de performance, nous sauvegardons cette version pour des étapes ultérieures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle Random Forest optimisé sauvegardé sous : note-books/phase_2_training/09_random_forest_advanced_optimization/Random_Forest_20_percent_optimized.joblib\n"
     ]
    }
   ],
   "source": [
    "# Chemin de sauvegarde pour le modèle optimisé\n",
    "model_path_optimized = f'{output_folder}/Random_Forest_20_percent_optimized.joblib'\n",
    "\n",
    "# Sauvegarder le modèle optimisé\n",
    "joblib.dump(best_model, model_path_optimized)\n",
    "print(f\"Modèle Random Forest optimisé sauvegardé sous : {model_path_optimized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Explication des étapes\n",
    "\n",
    "1. **Maintien de l'échantillon à 20%** : Nous conservons 20% des données pour le modèle afin de limiter la charge de calcul.\n",
    "2. **Optimisation avec RandomizedSearchCV** : Random Search nous permet d'explorer une large gamme d’hyperparamètres sans augmenter significativement le temps d’entraînement.\n",
    "3. **Évaluation avec validation croisée** : La validation croisée aide à obtenir des scores de performance robustes et à mesurer la stabilité des performances du modèle.\n",
    "4. **Vérification de seuil de performance** : Un test de seuil est effectué pour déterminer si le modèle répond aux exigences de performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Résumé\n",
    "\n",
    "Dans ce notebook, nous avons :\n",
    "1. Utilisé **RandomizedSearchCV** pour optimiser les hyperparamètres du modèle **Random Forest** avec un échantillon de 20%.\n",
    "2. Évalué les performances du modèle avec validation croisée pour obtenir des scores robustes.\n",
    "3. Vérifié si le modèle atteint le seuil de performance souhaité et sauvegardé le modèle si les résultats sont satisfaisants.\n",
    "\n",
    "Si le modèle atteint le niveau de performance souhaité (R² moyen ≥ 0.95), il est prêt pour être déployé ou pour des étapes supplémentaires de validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
